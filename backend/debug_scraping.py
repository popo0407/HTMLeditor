#!/usr/bin/env python3
"""
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å•é¡Œã®ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import asyncio
import sys
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from backend.app.services.scraping_service import ScrapingService
from backend.app.models.scraping_schemas import ScrapingRequest, LoginCredentials, ScrapingMode, UrlConfig

async def debug_scraping_issues():
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®å•é¡Œã‚’ãƒ‡ãƒãƒƒã‚°"""
    print("ğŸ” ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å•é¡Œã®ãƒ‡ãƒãƒƒã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
    
    try:
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–
        scraping_service = ScrapingService()
        print("âœ… ScrapingServiceåˆæœŸåŒ–å®Œäº†")
        
        # å˜ä¸€ã®URLã§ãƒ†ã‚¹ãƒˆï¼ˆtest_page1.htmlï¼‰
        test_request = ScrapingRequest(
            credentials=LoginCredentials(
                username="test_user",
                password="test_pass",
                login_url="http://localhost:8080/test_login.html"
            ),
            url_configs=[
                UrlConfig(
                    url="http://localhost:8080/test_page1.html",
                    mode=ScrapingMode.CHAT_ENTRIES
                )
            ]
        )
        
        print("âœ… ãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆå®Œäº†")
        print(f"   å¯¾è±¡URL: {test_request.url_configs[0].url}")
        print(f"   ãƒ¢ãƒ¼ãƒ‰: {test_request.url_configs[0].mode}")
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        print("\nğŸ“¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­...")
        response = await scraping_service.execute_scraping(test_request)
        
        print("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")
        print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {response.session_id}")
        print(f"   å‡¦ç†æ™‚é–“: {response.total_processing_time:.2f}ç§’")
        print(f"   çµæœæ•°: {len(response.results)}")
        
        # çµæœã®è©³ç´°è¡¨ç¤º
        for i, result in enumerate(response.results, 1):
            print(f"\nğŸ“Š çµæœ {i}:")
            print(f"   URL: {result.url}")
            print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.status}")
            print(f"   ãƒ¢ãƒ¼ãƒ‰: {result.mode}")
            if result.status == "success":
                print(f"   ãƒ‡ãƒ¼ã‚¿é•·: {len(result.data) if result.data else 0}æ–‡å­—")
                if result.data:
                    preview = result.data[:200] + "..." if len(result.data) > 200 else result.data
                    print(f"   ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {preview}")
            else:
                print(f"   ã‚¨ãƒ©ãƒ¼: {result.error_message}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            await scraping_service.shutdown()
            print("ğŸ§¹ ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        except Exception as e:
            print(f"âš ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    print("=" * 60)
    print("HTMLEditer ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å•é¡Œ ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # éåŒæœŸãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
    success = asyncio.run(debug_scraping_issues())
    
    if success:
        print("\nâœ… ãƒ‡ãƒãƒƒã‚°ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        sys.exit(0)
    else:
        print("\nâŒ ãƒ‡ãƒãƒƒã‚°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        sys.exit(1)
