#!/usr/bin/env python3
"""
ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆ

é–‹ç™ºãƒãƒ£ãƒ¼ã‚¿ãƒ¼ã«å¾“ã„ã€å®Ÿéš›ã®å‹•ä½œã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
test_page1.html ã¨ test_page2.html ã®ä¸¡æ–¹ã‚’ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import sys
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.app.services.scraping_service import ScrapingService, ScrapingMode
from backend.app.models.scraping_schemas import ScrapingRequest, ScrapingResult, LoginCredentials
from backend.app.config.settings import get_settings

async def test_page_scraping():
    """ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
    print("=" * 60)
    print("HTMLEditer ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        # è¨­å®šã‚’å–å¾—
        settings = get_settings()
        print("âœ… è¨­å®šå–å¾—å®Œäº†")
        
        # ScrapingServiceã‚’åˆæœŸåŒ–
        scraping_service = ScrapingService()
        print("âœ… ScrapingServiceåˆæœŸåŒ–å®Œäº†")
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
        test_urls = [
            "http://localhost:8080/test_page1.html",
            "http://localhost:8080/test_page2.html"
        ]
        
        # ãƒ€ãƒŸãƒ¼ã®èªè¨¼æƒ…å ±ã‚’ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        credentials = LoginCredentials(
            username="testuser",
            password="testpass",
            login_url="http://localhost:8080/test_login.html"
        )
        
        # å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚¹ãƒˆ
        for i, test_url in enumerate(test_urls, 1):
            print(f"\n{'='*50}")
            print(f"ğŸ“„ ãƒ†ã‚¹ãƒˆ {i}: {test_url}")
            print(f"{'='*50}")
            
            # ãƒšãƒ¼ã‚¸1ã¯ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒ¢ãƒ¼ãƒ‰ã€ãƒšãƒ¼ã‚¸2ã¯ã‚¿ã‚¤ãƒˆãƒ«æ—¥ä»˜å‚åŠ è€…ãƒ¢ãƒ¼ãƒ‰
            mode = ScrapingMode.CHAT_ENTRIES if "page1" in test_url else ScrapingMode.TITLE_DATE_PARTICIPANT
            
            request = ScrapingRequest(
                credentials=credentials,
                target_urls=[test_url],
                mode=mode
            )
            
            print(f"âœ… ãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆå®Œäº†")
            print(f"   å¯¾è±¡URL: {test_url}")
            print(f"   ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰: {mode}")
            
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
            print(f"\nğŸ“¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­...")
            results = await scraping_service.execute_scraping(request)
            
            # çµæœã‚’è¡¨ç¤º
            print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")
            print(f"   çµæœæ•°: {len(results.results)}")
            print(f"   ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {results.session_id}")
            print(f"   å‡¦ç†æ™‚é–“: {results.total_processing_time:.2f}ç§’")
            
            # çµæœã®è©³ç´°è¡¨ç¤º
            for j, result in enumerate(results.results, 1):
                print(f"\nğŸ” çµæœ {j}:")
                print(f"   URL: {result.url}")
                print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.status}")
                print(f"   ãƒ¢ãƒ¼ãƒ‰: {result.mode}")
                
                if result.status == "success":
                    print(f"   âœ… æˆåŠŸ")
                    print(f"   ãƒ‡ãƒ¼ã‚¿é•·: {len(result.data) if result.data else 0}æ–‡å­—")
                    
                    # ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®100æ–‡å­—ã‚’è¡¨ç¤º
                    if result.data:
                        preview = result.data[:100] + "..." if len(result.data) > 100 else result.data
                        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {preview}")
                else:
                    print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {result.error_message}")
        
        print(f"\nğŸ‰ ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    success = await test_page_scraping()
    if success:
        print("\nâœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ")
        sys.exit(0)
    else:
        print("\nâŒ ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
